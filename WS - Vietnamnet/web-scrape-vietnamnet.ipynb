{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "muc = 'thoi-su'\n",
    "max_page = 5542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title():\n",
    "    title_1 = soup2.find('div', class_ ='verticalPost__main')\n",
    "    if title_1 != None:\n",
    "        heading_2 = title_1.find('h2')\n",
    "        title1 = heading_2.find('a').get('title')\n",
    "        titles.append(title1)\n",
    "\n",
    "    title_2 = soup2.find_all('h3')\n",
    "    for tit in title_2:\n",
    "        title = tit.find('a').get('title')\n",
    "        titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category():\n",
    "    vert_cat = soup2.find_all('div', class_='verticalPost__main-cate')\n",
    "    if vert_cat != None:\n",
    "        for cat in vert_cat:\n",
    "            category1 = cat.find('a').get('title')\n",
    "            cate.append(category1)\n",
    "            \n",
    "    hor_cat = soup2.find_all('div', class_='horizontalPost__main-cate show')\n",
    "    for cat in hor_cat:\n",
    "        category2 = cat.find('a').get('title')\n",
    "        cate.append(category2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url():\n",
    "    for links in vertical_post:\n",
    "        anchor = links.find('a')\n",
    "        link = 'https://vietnamnet.vn/'+ anchor.attrs['href']\n",
    "        # link =  anchor.attrs['href']\n",
    "        urls.append(link)\n",
    "    for links in horizontal_post:\n",
    "        anchor = links.find('a')\n",
    "        link = 'https://vietnamnet.vn/' + anchor.attrs['href']\n",
    "        # link = anchor.attrs['href']\n",
    "        urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_2():\n",
    "    for links in vertical_post:\n",
    "        anchor = links.find('a')\n",
    "        # link = 'https://vietnamnet.vn/'+ anchor.attrs['href']\n",
    "        link =  anchor.attrs['href']\n",
    "        urls.append(link)\n",
    "    for links in horizontal_post:\n",
    "        anchor = links.find('a')\n",
    "        # link = 'https://vietnamnet.vn/' + anchor.attrs['href']\n",
    "        link = anchor.attrs['href']\n",
    "        urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_type_1():\n",
    "        # article = soup.find('div', class_ = 'container__left not-pl')\n",
    "        body = article.find(class_ = 'maincontent main-content').text\n",
    "        time = soup.find('div', class_ = 'bread-crumb-detail__time').text.strip()\n",
    "        date.append(time)\n",
    "        text.append(body)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_type_2():\n",
    "    time = soup.find('div', class_ = 'bread-crumb-detail__time').text.strip()\n",
    "    body = soup.find_all('p')\n",
    "    for txt in body: \n",
    "        text.append(txt.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame(columns= ['date', 'title', 'category', 'text', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang cào mục thoi-su\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print('Đang cào mục', muc)\n",
    "for page in tqdm(range(1, 4), ncols=100):\n",
    "    try:\n",
    "        titles = []\n",
    "        cate = []\n",
    "        urls = []\n",
    "        date = []\n",
    "        text = []\n",
    "        request = requests.get(f'https://vietnamnet.vn/{muc}-page{page}')\n",
    "        request = request.content\n",
    "        soup2 = BeautifulSoup(request,'html.parser')\n",
    "        vertical_post = soup2.find_all('div', class_ = 'verticalPost__avt')\n",
    "        horizontal_post = soup2.find_all('div', class_ = 'horizontalPost__avt avt-240')\n",
    "        # Get article titles\n",
    "        get_title()\n",
    "        # Get article categories \n",
    "        get_category()\n",
    "        # Get article links\n",
    "        get_url()\n",
    "        # Scrape an article\n",
    "        for url in urls:\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "                req = req.content\n",
    "                soup = BeautifulSoup(req, 'html.parser')\n",
    "                article = soup.find('div', class_ = 'container__left not-pl')\n",
    "                if article != None:\n",
    "                    try:\n",
    "                        get_article_type_1()   \n",
    "                    except:\n",
    "                        continue \n",
    "                else:\n",
    "                    try:\n",
    "                        get_article_type_2()\n",
    "                    except Exception as error:\n",
    "                        print(url, error)\n",
    "            except Exception as error:\n",
    "                        print(url, error)\n",
    "        news_list_1 = pd.DataFrame(list(zip(date, titles, cate, text, urls)), columns= ['date', 'title', 'category', 'text', 'url'])\n",
    "        news_df = pd.concat([news_df, news_list_1])\n",
    "    except Exception as error:\n",
    "         print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Đang cào mục', muc)\n",
    "for page in tqdm(range(4, max_page + 1), ncols=100):\n",
    "    try:\n",
    "        titles = []\n",
    "        cate = []\n",
    "        urls = []\n",
    "        date = []\n",
    "        text = []\n",
    "        request = requests.get(f'https://vietnamnet.vn/{muc}-page{page}')\n",
    "        request = request.content\n",
    "        soup2 = BeautifulSoup(request,'html.parser')\n",
    "        vertical_post = soup2.find_all('div', class_ = 'verticalPost__avt')\n",
    "        horizontal_post = soup2.find_all('div', class_ = 'horizontalPost__avt avt-240')\n",
    "        # Get article titles\n",
    "        get_title()\n",
    "        # Get article categories \n",
    "        get_category()\n",
    "        # Get article links\n",
    "        get_url_2()\n",
    "        # Scrape an article\n",
    "        for url in urls:\n",
    "            try:\n",
    "                req = requests.get(url)\n",
    "                req = req.content\n",
    "                soup = BeautifulSoup(req, 'html.parser')\n",
    "                article = soup.find('div', class_ = 'container__left not-pl')\n",
    "                if article != None:\n",
    "                    try:\n",
    "                        get_article_type_1()   \n",
    "                    except:\n",
    "                        continue \n",
    "                else:\n",
    "                    try:\n",
    "                        get_article_type_2()\n",
    "                    except Exception as error:\n",
    "                        print(url, error)\n",
    "            except Exception as error:\n",
    "                        print(url, error)\n",
    "        news_list_2 = pd.DataFrame(list(zip(date, titles, cate, text, urls)), columns= ['date', 'title', 'category', 'text', 'url'])\n",
    "        news_df = pd.concat([news_df, news_list_2])\n",
    "    except Exception as error:\n",
    "         print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_list = list(zip(date, titles, cate, text, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv(f'./vietnamnet-{muc}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
